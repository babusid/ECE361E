Epoch: [1/25], Step: [100/468], Loss: 0.3395 Acc: 89.09%
Epoch: [1/25], Step: [200/468], Loss: 0.2146 Acc: 93.23%
Epoch: [1/25], Step: [300/468], Loss: 0.1684 Acc: 94.71%
Epoch: [1/25], Step: [400/468], Loss: 0.1447 Acc: 95.43%
Test accuracy: 98.46 % Test loss: 0.0453
Epoch: [2/25], Step: [100/468], Loss: 0.0449 Acc: 98.54%
Epoch: [2/25], Step: [200/468], Loss: 0.0446 Acc: 98.57%
Epoch: [2/25], Step: [300/468], Loss: 0.0476 Acc: 98.54%
Epoch: [2/25], Step: [400/468], Loss: 0.0481 Acc: 98.52%
Test accuracy: 98.49 % Test loss: 0.0449
Epoch: [3/25], Step: [100/468], Loss: 0.0371 Acc: 98.81%
Epoch: [3/25], Step: [200/468], Loss: 0.0364 Acc: 98.83%
Epoch: [3/25], Step: [300/468], Loss: 0.0378 Acc: 98.82%
Epoch: [3/25], Step: [400/468], Loss: 0.0364 Acc: 98.83%
Test accuracy: 98.73 % Test loss: 0.0389
Epoch: [4/25], Step: [100/468], Loss: 0.0295 Acc: 99.08%
Epoch: [4/25], Step: [200/468], Loss: 0.0290 Acc: 99.07%
Epoch: [4/25], Step: [300/468], Loss: 0.0290 Acc: 99.11%
Epoch: [4/25], Step: [400/468], Loss: 0.0293 Acc: 99.11%
Test accuracy: 98.81 % Test loss: 0.0360
Epoch: [5/25], Step: [100/468], Loss: 0.0201 Acc: 99.36%
Epoch: [5/25], Step: [200/468], Loss: 0.0216 Acc: 99.36%
Epoch: [5/25], Step: [300/468], Loss: 0.0227 Acc: 99.30%
Epoch: [5/25], Step: [400/468], Loss: 0.0224 Acc: 99.30%
Test accuracy: 99.03 % Test loss: 0.0295
Epoch: [6/25], Step: [100/468], Loss: 0.0165 Acc: 99.53%
Epoch: [6/25], Step: [200/468], Loss: 0.0186 Acc: 99.47%
Epoch: [6/25], Step: [300/468], Loss: 0.0183 Acc: 99.46%
Epoch: [6/25], Step: [400/468], Loss: 0.0186 Acc: 99.43%
Test accuracy: 99.10 % Test loss: 0.0292
Epoch: [7/25], Step: [100/468], Loss: 0.0139 Acc: 99.58%
Epoch: [7/25], Step: [200/468], Loss: 0.0152 Acc: 99.56%
Epoch: [7/25], Step: [300/468], Loss: 0.0150 Acc: 99.56%
Epoch: [7/25], Step: [400/468], Loss: 0.0144 Acc: 99.60%
Test accuracy: 98.80 % Test loss: 0.0382
Epoch: [8/25], Step: [100/468], Loss: 0.0125 Acc: 99.66%
Epoch: [8/25], Step: [200/468], Loss: 0.0126 Acc: 99.68%
Epoch: [8/25], Step: [300/468], Loss: 0.0136 Acc: 99.63%
Epoch: [8/25], Step: [400/468], Loss: 0.0131 Acc: 99.62%
Test accuracy: 99.04 % Test loss: 0.0322
Epoch: [9/25], Step: [100/468], Loss: 0.0096 Acc: 99.76%
Epoch: [9/25], Step: [200/468], Loss: 0.0100 Acc: 99.77%
Epoch: [9/25], Step: [300/468], Loss: 0.0103 Acc: 99.74%
Epoch: [9/25], Step: [400/468], Loss: 0.0105 Acc: 99.72%
Test accuracy: 98.89 % Test loss: 0.0357
Epoch: [10/25], Step: [100/468], Loss: 0.0075 Acc: 99.85%
Epoch: [10/25], Step: [200/468], Loss: 0.0069 Acc: 99.84%
Epoch: [10/25], Step: [300/468], Loss: 0.0073 Acc: 99.83%
Epoch: [10/25], Step: [400/468], Loss: 0.0081 Acc: 99.80%
Test accuracy: 99.11 % Test loss: 0.0279
Epoch: [11/25], Step: [100/468], Loss: 0.0052 Acc: 99.92%
Epoch: [11/25], Step: [200/468], Loss: 0.0058 Acc: 99.89%
Epoch: [11/25], Step: [300/468], Loss: 0.0061 Acc: 99.88%
Epoch: [11/25], Step: [400/468], Loss: 0.0062 Acc: 99.88%
Test accuracy: 98.87 % Test loss: 0.0335
Epoch: [12/25], Step: [100/468], Loss: 0.0066 Acc: 99.82%
Epoch: [12/25], Step: [200/468], Loss: 0.0058 Acc: 99.84%
Epoch: [12/25], Step: [300/468], Loss: 0.0057 Acc: 99.87%
Epoch: [12/25], Step: [400/468], Loss: 0.0058 Acc: 99.88%
Test accuracy: 99.09 % Test loss: 0.0305
Epoch: [13/25], Step: [100/468], Loss: 0.0054 Acc: 99.94%
Epoch: [13/25], Step: [200/468], Loss: 0.0043 Acc: 99.95%
Epoch: [13/25], Step: [300/468], Loss: 0.0040 Acc: 99.95%
Epoch: [13/25], Step: [400/468], Loss: 0.0041 Acc: 99.94%
Test accuracy: 99.10 % Test loss: 0.0309
Epoch: [14/25], Step: [100/468], Loss: 0.0035 Acc: 99.95%
Epoch: [14/25], Step: [200/468], Loss: 0.0029 Acc: 99.96%
Epoch: [14/25], Step: [300/468], Loss: 0.0032 Acc: 99.95%
Epoch: [14/25], Step: [400/468], Loss: 0.0034 Acc: 99.95%
Test accuracy: 99.03 % Test loss: 0.0340
Epoch: [15/25], Step: [100/468], Loss: 0.0025 Acc: 99.97%
Epoch: [15/25], Step: [200/468], Loss: 0.0027 Acc: 99.98%
Epoch: [15/25], Step: [300/468], Loss: 0.0027 Acc: 99.98%
Epoch: [15/25], Step: [400/468], Loss: 0.0028 Acc: 99.98%
Test accuracy: 99.14 % Test loss: 0.0325
Epoch: [16/25], Step: [100/468], Loss: 0.0022 Acc: 99.98%
Epoch: [16/25], Step: [200/468], Loss: 0.0029 Acc: 99.96%
Epoch: [16/25], Step: [300/468], Loss: 0.0028 Acc: 99.96%
Epoch: [16/25], Step: [400/468], Loss: 0.0026 Acc: 99.96%
Test accuracy: 99.08 % Test loss: 0.0310
Epoch: [17/25], Step: [100/468], Loss: 0.0012 Acc: 100.00%
Epoch: [17/25], Step: [200/468], Loss: 0.0012 Acc: 100.00%
Epoch: [17/25], Step: [300/468], Loss: 0.0016 Acc: 99.99%
Epoch: [17/25], Step: [400/468], Loss: 0.0017 Acc: 99.99%
Test accuracy: 99.13 % Test loss: 0.0334
Epoch: [18/25], Step: [100/468], Loss: 0.0013 Acc: 99.99%
Epoch: [18/25], Step: [200/468], Loss: 0.0013 Acc: 100.00%
Epoch: [18/25], Step: [300/468], Loss: 0.0014 Acc: 99.99%
Epoch: [18/25], Step: [400/468], Loss: 0.0014 Acc: 99.99%
Test accuracy: 99.07 % Test loss: 0.0329
Epoch: [19/25], Step: [100/468], Loss: 0.0016 Acc: 99.98%
Epoch: [19/25], Step: [200/468], Loss: 0.0013 Acc: 99.99%
Epoch: [19/25], Step: [300/468], Loss: 0.0013 Acc: 99.99%
Epoch: [19/25], Step: [400/468], Loss: 0.0012 Acc: 99.99%
Test accuracy: 99.14 % Test loss: 0.0321
Epoch: [20/25], Step: [100/468], Loss: 0.0014 Acc: 99.98%
Epoch: [20/25], Step: [200/468], Loss: 0.0012 Acc: 99.99%
Epoch: [20/25], Step: [300/468], Loss: 0.0011 Acc: 99.99%
Epoch: [20/25], Step: [400/468], Loss: 0.0010 Acc: 99.99%
Test accuracy: 99.11 % Test loss: 0.0327
Epoch: [21/25], Step: [100/468], Loss: 0.0009 Acc: 100.00%
Epoch: [21/25], Step: [200/468], Loss: 0.0008 Acc: 100.00%
Epoch: [21/25], Step: [300/468], Loss: 0.0008 Acc: 100.00%
Epoch: [21/25], Step: [400/468], Loss: 0.0009 Acc: 100.00%
Test accuracy: 99.10 % Test loss: 0.0332
Epoch: [22/25], Step: [100/468], Loss: 0.0008 Acc: 100.00%
Epoch: [22/25], Step: [200/468], Loss: 0.0007 Acc: 100.00%
Epoch: [22/25], Step: [300/468], Loss: 0.0007 Acc: 100.00%
Epoch: [22/25], Step: [400/468], Loss: 0.0007 Acc: 100.00%
Test accuracy: 99.07 % Test loss: 0.0341
Epoch: [23/25], Step: [100/468], Loss: 0.0007 Acc: 100.00%
Epoch: [23/25], Step: [200/468], Loss: 0.0007 Acc: 100.00%
Epoch: [23/25], Step: [300/468], Loss: 0.0007 Acc: 100.00%
Epoch: [23/25], Step: [400/468], Loss: 0.0006 Acc: 100.00%
Test accuracy: 99.15 % Test loss: 0.0330
Epoch: [24/25], Step: [100/468], Loss: 0.0006 Acc: 100.00%
Epoch: [24/25], Step: [200/468], Loss: 0.0006 Acc: 100.00%
Epoch: [24/25], Step: [300/468], Loss: 0.0006 Acc: 100.00%
Epoch: [24/25], Step: [400/468], Loss: 0.0006 Acc: 100.00%
Test accuracy: 99.14 % Test loss: 0.0332
Epoch: [25/25], Step: [100/468], Loss: 0.0004 Acc: 100.00%
Epoch: [25/25], Step: [200/468], Loss: 0.0005 Acc: 100.00%
Epoch: [25/25], Step: [300/468], Loss: 0.0005 Acc: 100.00%
Epoch: [25/25], Step: [400/468], Loss: 0.0005 Acc: 100.00%
Test accuracy: 99.12 % Test loss: 0.0340
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
SimpleCNN                                [1, 10]                   --
├─Conv2d: 1-1                            [1, 32, 28, 28]           320
├─MaxPool2d: 1-2                         [1, 32, 14, 14]           --
├─BatchNorm2d: 1-3                       [1, 32, 14, 14]           64
├─Conv2d: 1-4                            [1, 64, 14, 14]           18,496
├─MaxPool2d: 1-5                         [1, 64, 7, 7]             --
├─Linear: 1-6                            [1, 10]                   31,370
==========================================================================================
Total params: 50,250
Trainable params: 50,250
Non-trainable params: 0
Total mult-adds (M): 3.91
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.35
Params size (MB): 0.20
Estimated Total Size (MB): 0.56
==========================================================================================
Saved Model size on disk: 199 kb
[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.
[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.
[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.
[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.
MACs: 3894912
FLOPs: 7789824
Finished
